<script lang="ts">
  import { onMount } from 'svelte';

  let mounted = $state(false);
  onMount(() => mounted = true);

  const article = {
    title: "I Made a Bot that Can Play the Wiki Game (Python code included!)",
    date: "January 18, 2024",
    categories: ["nlp", "transformers", "ai", "wikipedia", "data science"],
    mediumUrl: "https://medium.com/@kmaurinjones/how-i-made-a-bot-that-can-play-the-wiki-game-python-code-included-5d207254cf33"
  };
</script>

<svelte:head>
  <title>{article.title} | Kai Maurin-Jones</title>
  <meta name="description" content="{article.title} - Read on Kai Maurin-Jones' blog" />
</svelte:head>

<div class="min-h-screen {mounted ? 'opacity-100' : 'opacity-0'} transition-opacity duration-700 overflow-hidden">
  <article class="max-w-4xl mx-auto px-4 w-full">
    <!-- Back link -->
    <a href="/thoughts" class="inline-flex items-center text-taupe hover:text-terracotta transition-colors mb-8">
      <svg class="w-5 h-5 mr-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M15 19l-7-7 7-7"></path>
      </svg>
      Back to Thoughts
    </a>

    <!-- Article header -->
    <header class="mb-12">
      <h1 class="text-3xl lg:text-4xl font-bold text-primary mb-4">
        {article.title}
      </h1>
      <div class="flex flex-wrap items-center gap-4 text-taupe">
        <time datetime="{article.date}">{article.date}</time>
        {#if article.categories.length > 0}
          <span class="text-taupe/40">â€¢</span>
          <div class="flex flex-wrap gap-2">
            {#each article.categories as category}
              <span class="px-3 py-1 bg-caramel/10 text-taupe text-sm rounded-full">
                {category}
              </span>
            {/each}
          </div>
        {/if}
      </div>
    </header>

    <!-- Article content -->
    <div class="prose prose-lg max-w-none mb-12 overflow-hidden
                prose-headings:text-primary prose-headings:font-bold
                prose-p:text-taupe prose-p:leading-relaxed prose-p:mb-4
                prose-a:text-terracotta prose-a:no-underline hover:prose-a:underline
                prose-strong:text-primary prose-strong:font-semibold
                prose-em:text-taupe prose-em:italic
                prose-code:text-primary prose-code:bg-caramel/10 prose-code:px-1 prose-code:py-0.5 prose-code:rounded
                prose-pre:bg-primary/5 prose-pre:border prose-pre:border-taupe/20
                prose-blockquote:border-l-4 prose-blockquote:border-terracotta prose-blockquote:pl-6 prose-blockquote:italic prose-blockquote:my-6
                prose-ul:text-taupe prose-ul:my-6 prose-ul:list-disc prose-ul:pl-6
                prose-ol:text-taupe prose-ol:my-6 prose-ol:list-decimal prose-ol:pl-6
                prose-li:text-taupe prose-li:my-2
                prose-img:rounded-lg prose-img:shadow-lg prose-img:my-8 prose-img:max-w-full prose-img:h-auto
                prose-h1:mb-4 prose-h1:mt-8
                prose-h2:mb-4 prose-h2:mt-8
                prose-h3:mb-3 prose-h3:mt-6
                prose-h4:mb-2 prose-h4:mt-4
                ">
      {@html "<article><div class=\"m\"><div class=\"m\"><span class=\"m\"></span><section><div><div class=\"er ge gf gg gh gi\"></div><div class=\"gj gk gl gm gn\"><div class=\"ac ci\"><div class=\"cp bi fv fw fx fy\"><div><div></div></div><p class=\"pw-post-body-paragraph ly lz gq ma b mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv gj bl\" id=\"4efb\"></p><figure class=\"na nb nc nd ne nf mx my paragraph-image\"><div class=\"mx my mz\"><picture><source sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 618px\" srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*Bo88uKJJvpVk_7kNLZkImg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*Bo88uKJJvpVk_7kNLZkImg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*Bo88uKJJvpVk_7kNLZkImg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*Bo88uKJJvpVk_7kNLZkImg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*Bo88uKJJvpVk_7kNLZkImg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Bo88uKJJvpVk_7kNLZkImg.png 1100w, https://miro.medium.com/v2/resize:fit:1236/format:webp/1*Bo88uKJJvpVk_7kNLZkImg.png 1236w\" type=\"image/webp\"/><source data-testid=\"og\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 618px\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*Bo88uKJJvpVk_7kNLZkImg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*Bo88uKJJvpVk_7kNLZkImg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*Bo88uKJJvpVk_7kNLZkImg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*Bo88uKJJvpVk_7kNLZkImg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*Bo88uKJJvpVk_7kNLZkImg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*Bo88uKJJvpVk_7kNLZkImg.png 1100w, https://miro.medium.com/v2/resize:fit:1236/1*Bo88uKJJvpVk_7kNLZkImg.png 1236w\"/><img alt=\"\" class=\"bi li ng c\" height=\"617\" loading=\"eager\" role=\"presentation\" width=\"618\"/></picture></div><figcaption class=\"nh ni nj mx my nk nl bg b bh ab ee\">Image source: Dall-E 2, prompt: \u201cWikigame bot\u201d</figcaption></figure><ul class=\"\"><li class=\"ly lz gq ma b mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv nm nn no bl\" id=\"e865\">Project Streamlit app: <a class=\"ah np\" href=\"https://wikigamebot.streamlit.app/\" rel=\"noopener ugc nofollow\" target=\"_blank\">https://wikigamebotbot.streamlit.app/</a></li><li class=\"ly lz gq ma b mb nq md me mf nr mh mi mj ns ml mm mn nt mp mq mr nu mt mu mv nm nn no bl\" id=\"64dd\">Project Github Repository: <a class=\"ah np\" href=\"https://github.com/kmaurinjones/WikiGameBot/tree/main\" rel=\"noopener ugc nofollow\" target=\"_blank\">https://github.com/kmaurinjones/WikiGameBot/tree/main</a></li><li class=\"ly lz gq ma b mb nq md me mf nr mh mi mj ns ml mm mn nt mp mq mr nu mt mu mv nm nn no bl\" id=\"841a\">Email: <a class=\"ah np\" href=\"mailto:kmaurinjones@gmail.com\" rel=\"noopener ugc nofollow\" target=\"_blank\">kmaurinjones@gmail.com</a></li><li class=\"ly lz gq ma b mb nq md me mf nr mh mi mj ns ml mm mn nt mp mq mr nu mt mu mv nm nn no bl\" id=\"26a7\">LinkedIn: <a class=\"ah np\" href=\"https://www.linkedin.com/in/kmaurinjones/\" rel=\"noopener ugc nofollow\" target=\"_blank\">https://www.linkedin.com/in/kmaurinjones/</a></li></ul><h2 class=\"nv nw gq bg nx ny nz oa ob oc od oe of og oh oi oj ok ol om on oo op oq or os bl\" id=\"a745\">Table of Contents</h2><p class=\"pw-post-body-paragraph ly lz gq ma b mb ot md me mf ou mh mi mj ov ml mm mn ow mp mq mr ox mt mu mv gj bl\" id=\"8de7\">If you\u2019re already familiar with APIs, Python, and NLP basics like Word Embeddings, you can skip the conceptual overview of most of this article. Here are the topics covered, so feel free to skip to that which interests you most.</p><p class=\"pw-post-body-paragraph ly lz gq ma b mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv gj bl\" id=\"beed\"><a class=\"ah np\" href=\"#c7f6\" rel=\"noopener ugc nofollow\"><strong class=\"ma gr\">Introduction</strong></a></p><p class=\"pw-post-body-paragraph ly lz gq ma b mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv gj bl\" id=\"e0eb\"><a class=\"ah np\" href=\"#f8d5\" rel=\"noopener ugc nofollow\"><strong class=\"ma gr\">Breaking Down the Problem</strong></a></p><p class=\"pw-post-body-paragraph ly lz gq ma b mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv gj bl\" id=\"ef1e\"><a class=\"ah np\" href=\"#6ee1\" rel=\"noopener ugc nofollow\"><strong class=\"ma gr\">Python Code Implementation</strong></a></p><p class=\"pw-post-body-paragraph ly lz gq ma b mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv gj bl\" id=\"75d2\"><a class=\"ah np\" href=\"#bf3d\" rel=\"noopener ugc nofollow\"><strong class=\"ma gr\">Start &amp; Target Topic Pages</strong></a></p><p class=\"pw-post-body-paragraph ly lz gq ma b mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv gj bl\" id=\"71f3\"><a class=\"ah np\" href=\"#c8ec\" rel=\"noopener ugc nofollow\"><strong class=\"ma gr\">Page Summaries</strong></a></p><p class=\"pw-post-body-paragraph ly lz gq ma b mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv gj bl\" id=\"9adf\"><a class=\"ah np\" href=\"#331d\" rel=\"noopener ugc nofollow\"><strong class=\"ma gr\">Improving Efficiency</strong></a></p><p class=\"pw-post-body-paragraph ly lz gq ma b mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv gj bl\" id=\"a2c9\"><a class=\"ah np\" href=\"#9a2a\" rel=\"noopener ugc nofollow\"><strong class=\"ma gr\">Validating Page Topics</strong></a></p><p class=\"pw-post-body-paragraph ly lz gq ma b mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv gj bl\" id=\"508c\"><a class=\"ah np\" href=\"#bce0\" rel=\"noopener ugc nofollow\"><strong class=\"ma gr\">Comparing Topics Using Cosine Similarity &amp; Sentence Transformer Embeddings Model</strong></a></p><p class=\"pw-post-body-paragraph ly lz gq ma b mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv gj bl\" id=\"8579\"><a class=\"ah np\" href=\"#53e9\" rel=\"noopener ugc nofollow\"><strong class=\"ma gr\">Re-Ranking Summary Embeddings</strong></a></p><p class=\"pw-post-body-paragraph ly lz gq ma b mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv gj bl\" id=\"28ad\">(<a class=\"ah np\" href=\"#95a4\" rel=\"noopener ugc nofollow\"><strong class=\"ma gr\">While) Looping Until Complete</strong></a></p></div></div></div><div class=\"ac ci oy oz pa pb\" role=\"separator\"><span class=\"pc by bn pd pe pf\"></span><span class=\"pc by bn pd pe pf\"></span><span class=\"pc by bn pd pe\"></span></div><div class=\"gj gk gl gm gn\"><div class=\"ac ci\"><div class=\"cp bi fv fw fx fy\"><h2 class=\"nv nw gq bg nx ny pg oa ob oc ph oe of og pi oi oj ok pj om on oo pk oq or os bl\" id=\"c7f6\"><strong class=\"an\">Introduction</strong></h2><p class=\"pw-post-body-paragraph ly lz gq ma b mb ot md me mf ou mh mi mj ov ml mm mn ow mp mq mr ox mt mu mv gj bl\" id=\"54b5\">The <a class=\"ah np\" href=\"https://en.wikipedia.org/wiki/Wikipedia:Wiki_Game#:~:text='2%2Dplayer%20Challenge'%3A%20The,to%20give%20the%20other%20player\" rel=\"noopener ugc nofollow\" target=\"_blank\">Wiki Game</a> was all the rage when I was in grade school. When we were lucky enough to get time in the school\u2019s computer lab, we spent our time doing things like the Wiki Game instead of doing the work we were supposed to be doing.</p><p class=\"pw-post-body-paragraph ly lz gq ma b mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv gj bl\" id=\"9ae4\">If this game wasn\u2019t an instrumental part of your childhood, let me explain it to you. You and the person or people you\u2019re competing against pick two Wikipedia articles, and, starting at the same time, you race to see who can get from the starting article to the target article in the shortest time, only navigating away from the current page by clicking one of the <a class=\"ah np\" href=\"https://en.wikipedia.org/wiki/Hyperlink\" rel=\"noopener ugc nofollow\" target=\"_blank\">hyperlinks</a> on the current page. More rules include things like not being allowed to revisit links, which forces the player to be more careful to not corner themselves into a page without many hyperlinks (this is the worst!).</p><p class=\"pw-post-body-paragraph ly lz gq ma b mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv gj bl\" id=\"4251\">This game popped into my head again, recently, and given the power of modern AI (primarily <a class=\"ah np\" href=\"https://en.wikipedia.org/wiki/Word_embedding\" rel=\"noopener ugc nofollow\" target=\"_blank\">word embeddings</a>), I thought this was a perfect challenge for me given my knowledge of <a class=\"ah np\" href=\"https://en.wikipedia.org/wiki/Natural_language_processing\" rel=\"noopener ugc nofollow\" target=\"_blank\">Natural Language Processing</a> and <a class=\"ah np\" href=\"https://en.wikipedia.org/wiki/Computational_linguistics\" rel=\"noopener ugc nofollow\" target=\"_blank\">Computational Linguistics</a>.</p></div></div></div><div class=\"ac ci oy oz pa pb\" role=\"separator\"><span class=\"pc by bn pd pe pf\"></span><span class=\"pc by bn pd pe pf\"></span><span class=\"pc by bn pd pe\"></span></div><div class=\"gj gk gl gm gn\"><div class=\"ac ci\"><div class=\"cp bi fv fw fx fy\"><h2 class=\"nv nw gq bg nx ny pg oa ob oc ph oe of og pi oi oj ok pj om on oo pk oq or os bl\" id=\"f8d5\"><strong class=\"an\">Breaking Down the Problem</strong></h2><p class=\"pw-post-body-paragraph ly lz gq ma b mb ot md me mf ou mh mi mj ov ml mm mn ow mp mq mr ox mt mu mv gj bl\" id=\"b898\">The best way to tackle anything is to break it down into its most basic components \u2014 especially when it comes to coding. Let\u2019s think about exactly what is going on in the Wiki Game, as you\u2019re sitting at your computer playing it.</p><ol class=\"\"><li class=\"ly lz gq ma b mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv pl nn no bl\" id=\"4be2\">Decide on a starting page and a target page</li><li class=\"ly lz gq ma b mb nq md me mf nr mh mi mj ns ml mm mn nt mp mq mr nu mt mu mv pl nn no bl\" id=\"9c22\">So long as your current page is not the target page (i.e. you have completed the objective), look at all hyperlinks on the current page, assess which might be the most relevant to the target topic (based on whatever context you may implicitly have, or have been explicitly given by the article), and follow that link to the next t page.</li><li class=\"ly lz gq ma b mb nq md me mf nr mh mi mj ns ml mm mn nt mp mq mr nu mt mu mv pl nn no bl\" id=\"481e\">Repeat step 2 until the target page is reached.</li></ol><p class=\"pw-post-body-paragraph ly lz gq ma b mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv gj bl\" id=\"ced7\">Pretty simple, isn\u2019t it?<strong class=\"ma gr\"> Let\u2019s break these ideas down</strong> into how we can put this together <strong class=\"ma gr\">using code</strong>.</p><ol class=\"\"><li class=\"ly lz gq ma b mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv pl nn no bl\" id=\"e2f2\">Get a starting page URL and a target page URL. They each need to be unique Wikipedia links that are not equal (there is no game if they\u2019re the same page)</li></ol><p class=\"pw-post-body-paragraph ly lz gq ma b mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv gj bl\" id=\"4135\">2. Next, we\u2019ll use a <a class=\"ah np\" href=\"https://www.w3schools.com/python/python_while_loops.asp\" rel=\"noopener ugc nofollow\" target=\"_blank\"><em class=\"mw\">while loop</em></a> to repeat the following series of steps on each current page:</p><ul class=\"\"><li class=\"ly lz gq ma b mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv nm nn no bl\" id=\"66f2\">get a summary of the starting topic and the target topic. I use Wikipedia\u2019s <em class=\"mw\">summary</em> instead of just the topic name text because having more text means that we <em class=\"mw\">should</em> have a more \u2018accurate\u2019 word embedding (especially since the meaning of one homonym vs. another cannot be distinguished with only the term text \u2014 context text helps to mitigate this)</li><li class=\"ly lz gq ma b mb nq md me mf nr mh mi mj ns ml mm mn nt mp mq mr nu mt mu mv nm nn no bl\" id=\"d9ea\">get all links on the page, and the topic text associated with each link. Our word embeddings model was trained on natural language text, so if we can get \u201cmarathon runner\u201d instead of \u201cmarathon_runner\u201d, it stands to reason that the embedding should have a more \u2018accurate\u2019 representation of the encoded meaning)</li><li class=\"ly lz gq ma b mb nq md me mf nr mh mi mj ns ml mm mn nt mp mq mr nu mt mu mv nm nn no bl\" id=\"1658\">rank all topics found on the page by their similarity to the target topic. To do this, I used <a class=\"ah np\" href=\"https://www.sciencedirect.com/topics/computer-science/cosine-similarity#:~:text=Cosine%20similarity%20measures%20the%20similarity,document%20similarity%20in%20text%20analysis.\" rel=\"noopener ugc nofollow\" target=\"_blank\">Cosine Similarity</a> (1 \u2014 <a class=\"ah np\" data-discover=\"true\" href=\"/@milana.shxanukova15/cosine-distance-and-cosine-similarity-a5da0e4d9ded\" rel=\"noopener\">Cosine Distance</a>; I used the <a class=\"ah np\" href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">Scipy module</a>\u2019s implementation of this). This will give us a list of all of our current page\u2019s topics, ordered from most related to the topic, to least.</li></ul><blockquote class=\"pm pn po\"><p class=\"ly lz mw ma b mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv gj bl\" id=\"92d6\"><strong class=\"ma gr\">Optional</strong>: get the summaries of the top n-related topics. As mentioned before, the more text we can encode in each word embedding, the more \u2018representative\u2019 our embedding will be of the meaning of that particular topic (to a point). This can improve semantic connections, and help to disambiguate possible difficulties like homonyms.</p></blockquote><ul class=\"\"><li class=\"ly lz gq ma b mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv nm nn no bl\" id=\"9eff\">with our current page topics sorted by relation to the target topic, we can simply pick the first list item/topic as our next topic, and let the while loop reach its next iteration. With any luck, after a few iterations, you should see the Cosine Similarity to the target topic steadily increasing. Also, pro tip: don\u2019t forget to keep a running log of all visited pages, and exclude these from the pages that are valid choices from future pages. This will stop your while loop from running infinitely.</li></ul></div></div></div><div class=\"ac ci oy oz pa pb\" role=\"separator\"><span class=\"pc by bn pd pe pf\"></span><span class=\"pc by bn pd pe pf\"></span><span class=\"pc by bn pd pe\"></span></div><div class=\"gj gk gl gm gn\"><div class=\"ac ci\"><div class=\"cp bi fv fw fx fy\"><h2 class=\"nv nw gq bg nx ny pg oa ob oc ph oe of og pi oi oj ok pj om on oo pk oq or os bl\" id=\"6ee1\"><strong class=\"an\">Python Code Implementation</strong></h2><p class=\"pw-post-body-paragraph ly lz gq ma b mb ot md me mf ou mh mi mj ov ml mm mn ow mp mq mr ox mt mu mv gj bl\" id=\"cb66\">Let\u2019s take a look at how I\u2019ve coded what we just went through. However, before we do, here are the versions of the software I\u2019ve used. Using these same versions of each should ensure your code can run as mine has.</p><ul class=\"\"><li class=\"ly lz gq ma b mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv nm nn no bl\" id=\"7c67\">Python==3.11.4</li><li class=\"ly lz gq ma b mb nq md me mf nr mh mi mj ns ml mm mn nt mp mq mr nu mt mu mv nm nn no bl\" id=\"7155\">wikipediaapi==0.6.0</li><li class=\"ly lz gq ma b mb nq md me mf nr mh mi mj ns ml mm mn nt mp mq mr nu mt mu mv nm nn no bl\" id=\"48ba\">requests==2.31.0</li><li class=\"ly lz gq ma b mb nq md me mf nr mh mi mj ns ml mm mn nt mp mq mr nu mt mu mv nm nn no bl\" id=\"c478\">BeautifulSoup==4.12.2</li><li class=\"ly lz gq ma b mb nq md me mf nr mh mi mj ns ml mm mn nt mp mq mr nu mt mu mv nm nn no bl\" id=\"350e\">Sentence Transformers==2.2.2</li><li class=\"ly lz gq ma b mb nq md me mf nr mh mi mj ns ml mm mn nt mp mq mr nu mt mu mv nm nn no bl\" id=\"0f8b\">NumPy==1.24.3</li><li class=\"ly lz gq ma b mb nq md me mf nr mh mi mj ns ml mm mn nt mp mq mr nu mt mu mv nm nn no bl\" id=\"3717\">SciPy==1.11.1</li><li class=\"ly lz gq ma b mb nq md me mf nr mh mi mj ns ml mm mn nt mp mq mr nu mt mu mv nm nn no bl\" id=\"3e08\">Pandas==2.0.3</li></ul><p class=\"pw-post-body-paragraph ly lz gq ma b mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv gj bl\" id=\"5d7c\">Except for getting the start and target Wiki articles, all wiki data from each article is retrieved using the <a class=\"ah np\" href=\"https://pypi.org/project/Wikipedia-API/\" rel=\"noopener ugc nofollow\" target=\"_blank\">Python Wikipedia API wrapper</a>. This is a very handy and easy-to-use method of getting data from the website. The PyPI documentation linked above is very clear and easy to follow, but I\u2019ll still explain every step in as much conceptual detail as I can.</p><h3 class=\"pp nw gq bg nx pq pr ps ob pt pu pv of mj pw px py mn pz qa qb mr qc qd qe qf bl\" id=\"bf3d\">Start &amp; Target Topic Pages</h3><p class=\"pw-post-body-paragraph ly lz gq ma b mb ot md me mf ou mh mi mj ov ml mm mn ow mp mq mr ox mt mu mv gj bl\" id=\"0e0d\">By using the Requests and BeautifulSoup libraries for this, I could leverage Wikipedia\u2019s search recommendation system instead of having to create one from scratch. I wanted the topic input for this to be as flexible as possible, so that the user could simply enter a topic, and the most relevant Wikipedia article would be returned, even if it didn\u2019t match the spelling of the input verbatim. For example, when passing \u201cbolognese\u201d, this method returns the Wikipedia page \u201cbolognese sauce\u201d, as a page for exactly \u201cbolognese\u201d doesn\u2019t exist.</p><p class=\"pw-post-body-paragraph ly lz gq ma b mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv gj bl\" id=\"fc00\">The logic of this is straightforward: Wikipedia has a search page with a consistent URL scheme. It constructs a URL for a Wikipedia search by appending the search term, with spaces replaced by \u2018+\u2019 signs (as is Wikipedia\u2019s convention), to a base URL. The function then sends an HTTP GET request to this URL using the <code class=\"de qg qh qi qj b\">requests</code> library and parses the returned HTML content using BeautifulSoup \u2014 a Python library for pulling data out of HTML and XML files. The BeautifulSoup object (<code class=\"de qg qh qi qj b\">soup</code>) is then used to find the first <code class=\"de qg qh qi qj b\">div</code> element with a class of <code class=\"de qg qh qi qj b\">\"mw-search-result-heading\"</code>, which contains the title of the most relevant Wikipedia search result. The function extracts the 'href' attribute of the first anchor (<code class=\"de qg qh qi qj b\">a</code>) tag inside this <code class=\"de qg qh qi qj b\">div</code>, which is the URL of the Wikipedia page. It then removes the \"/wiki/\" part from this URL, which is a common prefix in Wikipedia page URLs, and returns the cleaned-up page title. The <code class=\"de qg qh qi qj b\">strip()</code> function at the end is used to remove any leading or trailing whitespace from the page title.</p><pre class=\"na nb nc nd ne qk qj ql bq qm bc bl\"><span class=\"qn nw gq qj b bh qo qp m qq qr\" id=\"9d27\">import requests<br/>from bs4 import BeautifulSoup<br/><br/>def search_wiki(search_term):<br/>    \"\"\"<br/>    Performs a Wikipedia search for a given term and retrieves the URL of the top search result.<br/><br/>    This function constructs a search URL for Wikipedia based on the provided search term. It then makes <br/>    a GET request to Wikipedia using this URL and parses the resulting HTML content to find the top search <br/>    result. The function extracts and returns the title of the Wikipedia page corresponding to this top result, <br/>    which can be used to construct a full URL or for further processing.<br/><br/>    Parameters<br/>    ----------<br/>    search_term : str<br/>        The term to be searched on Wikipedia. The search term is split by whitespace into words,<br/>        and '+' symbols are used to join these words in the search URL.<br/><br/>    Returns<br/>    -------<br/>    str<br/>        The title of the Wikipedia page corresponding to the top search result. The title is extracted from <br/>        the URL of the top result and is returned in a format that can be appended to 'https://en.wikipedia.org/wiki/' <br/>        to form the full URL of the page.<br/>    \"\"\"<br/>    search_url = f\"https://en.wikipedia.org/w/index.php?search={'+'.join(search_term.strip().split())}&amp;title=Special:Search&amp;profile=advanced&amp;fulltext=1&amp;ns0=1\"<br/>    soup = BeautifulSoup(requests.get(search_url).content, \"html.parser\")<br/>    return soup.find(\"div\", class_ = \"mw-search-result-heading\").a['href'].replace(\"/wiki/\", \"\").strip()<br/><br/>### Example:<br/># search_wiki(\"bolognese\")<br/># &gt;&gt;&gt; \"Bolognese_sauce\" # if we add this to \"/en.wikipedia.org/wiki/\", we get the article for this topic</span></pre><p class=\"pw-post-body-paragraph ly lz gq ma b mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv gj bl\" id=\"ebd5\">With the exact Wiki articles of each of these topics, everything that follows will exclusively use the Wikipedia API to get data.</p><h3 class=\"pp nw gq bg nx pq pr ps ob pt pu pv of mj pw px py mn pz qa qb mr qc qd qe qf bl\" id=\"c8ec\">Page Summaries</h3><p class=\"pw-post-body-paragraph ly lz gq ma b mb ot md me mf ou mh mi mj ov ml mm mn ow mp mq mr ox mt mu mv gj bl\" id=\"1a4d\">With the starting and target topics in-hand, we\u2019re going to gather some more context for each topic by getting Wikipedia\u2019s \u2018summary\u2019 of each topic from the topic\u2019s page. <a class=\"ah np\" href=\"https://pypi.org/project/Wikipedia-API/\" rel=\"noopener ugc nofollow\" target=\"_blank\">The API</a> has very good documentation on how to do this. Simply call the <code class=\"de qg qh qi qj b\">summary</code> attribute of the <code class=\"de qg qh qi qj b\">WikipediaPage</code> class, which returns a string of text of the summary.</p><pre class=\"na nb nc nd ne qk qj ql bq qm bc bl\"><span class=\"qn nw gq qj b bh qo qp m qq qr\" id=\"18af\">import wikipediaapi<br/><br/>wiki_article = 'Python_(programming_language)'<br/>page_py = wiki_wiki.page(wiki_article)<br/>page_py.summary[0:60]) # only printing the first 60 characters of the string<br/># &gt;&gt;&gt; 'Python is a widely used high-level programming language for'</span></pre><p class=\"pw-post-body-paragraph ly lz gq ma b mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv gj bl\" id=\"ebc2\">We can now get enough context for each topic to make a more semantically dense word embedding, which should lead to better performance when comparing embeddings.</p><h3 class=\"pp nw gq bg nx pq pr ps ob pt pu pv of mj pw px py mn pz qa qb mr qc qd qe qf bl\" id=\"b91f\">Page Topics &amp; Links</h3><p class=\"pw-post-body-paragraph ly lz gq ma b mb ot md me mf ou mh mi mj ov ml mm mn ow mp mq mr ox mt mu mv gj bl\" id=\"ddf9\">To get from one page to another, we need to see what linked topics (hyperlinks) there are on the current page. To do so, we can simply call the <code class=\"de qg qh qi qj b\">links</code> attribute on the <code class=\"de qg qh qi qj b\">WikipediaPage</code> object, which returns a Python dictionary of <code class=\"de qg qh qi qj b\">{link: link info}</code> pairs, from which we can use the BuiltIn dictionary method <code class=\"de qg qh qi qj b\">.keys()</code> to get just the topic names of all hyperlinks on the page.</p><pre class=\"na nb nc nd ne qk qj ql bq qm bc bl\"><span class=\"qn nw gq qj b bh qo qp m qq qr\" id=\"9ecb\">wiki_article = 'Python_(programming_language)'<br/>page_py = wiki_wiki.page(wiki_article)<br/>page_py.links.keys()<br/># &gt;&gt;&gt; dict_keys(['\"Hello, World!\" program', '3ds Max', ... ])</span></pre><p class=\"pw-post-body-paragraph ly lz gq ma b mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv gj bl\" id=\"1c63\">As each of these is themself a Wikipedia article name, we know that we can pass any of the strings to the <code class=\"de qg qh qi qj b\">WikipediaPage</code> <code class=\"de qg qh qi qj b\">page()</code> method, and repeat the process as many times as we\u2019d like.</p><pre class=\"na nb nc nd ne qk qj ql bq qm bc bl\"><span class=\"qn nw gq qj b bh qo qp m qq qr\" id=\"f569\">next_topic = '\"Hello, World!\" program'<br/>page_py = wiki_wiki.page(next_topic)<br/>page_py.summary       # returns summary for this page<br/>page_py.links.keys()  # returns dict_keys object of topics</span></pre><p class=\"pw-post-body-paragraph ly lz gq ma b mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv gj bl\" id=\"4f48\">This is how we can achieve the mechanic of navigating from page to page, but to do so effectively, there are a few other considerations that we can take into account.</p></div></div></div><div class=\"ac ci oy oz pa pb\" role=\"separator\"><span class=\"pc by bn pd pe pf\"></span><span class=\"pc by bn pd pe pf\"></span><span class=\"pc by bn pd pe\"></span></div><div class=\"gj gk gl gm gn\"><div class=\"ac ci\"><div class=\"cp bi fv fw fx fy\"><h2 class=\"nv nw gq bg nx ny pg oa ob oc ph oe of og pi oi oj ok pj om on oo pk oq or os bl\" id=\"331d\">Improving Efficiency</h2><p class=\"pw-post-body-paragraph ly lz gq ma b mb ot md me mf ou mh mi mj ov ml mm mn ow mp mq mr ox mt mu mv gj bl\" id=\"8993\">When discussing code efficiency, we need to think about how long each step of the process takes.</p><ul class=\"\"><li class=\"ly lz gq ma b mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv nm nn no bl\" id=\"0466\">Getting start/target topics -&gt; depends on internet speed and server response time</li><li class=\"ly lz gq ma b mb nq md me mf nr mh mi mj ns ml mm mn nt mp mq mr nu mt mu mv nm nn no bl\" id=\"8956\">Getting information (page title, page links, page summary) from each Wikipedia Page via the API -&gt; <strong class=\"ma gr\">depends on API speed</strong></li><li class=\"ly lz gq ma b mb nq md me mf nr mh mi mj ns ml mm mn nt mp mq mr nu mt mu mv nm nn no bl\" id=\"7011\">Generating Word Embeddings &amp; calculating Cosine Similarities -&gt; Runs as quickly as possible depending on local hardware</li></ul><p class=\"pw-post-body-paragraph ly lz gq ma b mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv gj bl\" id=\"fec1\">Given the above, the biggest bottleneck in the code is the time it takes the API to retrieve the information we request. Since we can\u2019t control the time each request takes, what we can control is how much information we request from it. We can use this knowledge to make things a bit more efficient.</p><h3 class=\"pp nw gq bg nx pq pr ps ob pt pu pv of mj pw px py mn pz qa qb mr qc qd qe qf bl\" id=\"9a2a\">Validating Page Topics</h3><p class=\"pw-post-body-paragraph ly lz gq ma b mb ot md me mf ou mh mi mj ov ml mm mn ow mp mq mr ox mt mu mv gj bl\" id=\"5115\">If you\u2019ve ever played this game, you might remember occasionally cornering yourself into pages such as <a class=\"ah np\" href=\"https://foundation.wikimedia.org/wiki/Category:Terms_of_Use\" rel=\"noopener ugc nofollow\" target=\"_blank\">https://foundation.wikimedia.org/wiki/Category:Terms_of_Use</a>, which effectively kill your run. This is a fun mechanic as a person competing against others, but for this bot, this only makes things annoying. In developing this framework, I noticed that several pages like this had consistent page prefixes, such as <em class=\"mw\">\u201cList of\u201d, \u201cHistory of\u201d, \u201cTemplate:\u201d, \u201cWikipedia:\u201d, \u201cCategory:\u201d, \u201cPortal:\u201d, \u201cTalk:\u201d, \u201cTemplate talk:\u201d</em>. By removing any links that start with these strings, we can increase the quality of the data seen downstream in this process. This is called <a class=\"ah np\" href=\"https://corporatefinanceinstitute.com/resources/data-science/data-validation/\" rel=\"noopener ugc nofollow\" target=\"_blank\">Data Validation</a>. You can add as many criteria here as you find reasonable or necessary. Validation criteria can be created by heuristics, by empirical findings, or typically by a combination of the two.</p><h3 class=\"pp nw gq bg nx pq pr ps ob pt pu pv of mj pw px py mn pz qa qb mr qc qd qe qf bl\" id=\"bce0\">Comparing Topics Using Cosine Similarity &amp; Sentence Transformer Embeddings Model</h3><p class=\"pw-post-body-paragraph ly lz gq ma b mb ot md me mf ou mh mi mj ov ml mm mn ow mp mq mr ox mt mu mv gj bl\" id=\"0554\">What is now commonplace in NLP is Word Embeddings. If you haven\u2019t seen or used them before, the main thing to understand about them is that they are essentially an attempt to quantify semantic information (or <em class=\"mw\">meaning</em>) in a given text. There are various <a class=\"ah np\" href=\"https://huggingface.co/blog/getting-started-with-embeddings\" rel=\"noopener ugc nofollow\" target=\"_blank\">Word Embedding models</a>, and though there are certain \u2018universal\u2019 embedding models, models are often made from scratch or are fine-tuned depending on the task. Word Embeddings propose two main values compared to simply looking at text:</p><ul class=\"\"><li class=\"ly lz gq ma b mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv nm nn no bl\" id=\"abd8\">They aim to capture <em class=\"mw\">meaning</em>, as opposed to just spelling. Comparing the relationship of two topics has little to do with their spellings, and much more to do with the meaning they symbolize</li><li class=\"ly lz gq ma b mb nq md me mf nr mh mi mj ns ml mm mn nt mp mq mr nu mt mu mv nm nn no bl\" id=\"5ab8\">As the information is <em class=\"mw\">quantified</em>, we can perform mathematical operations on embeddings, which makes comparisons like Cosine Distance computationally fast to perform, and logically simple to do</li></ul><p class=\"pw-post-body-paragraph ly lz gq ma b mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv gj bl\" id=\"1901\">In this task, we are taking 1) a reference topic, and 2) a list of candidate topics, comparing the embedding of each candidate topic to that of the reference, and seeing for which topic\u2019s embedding the Cosine Similarity is highest (this is synonymous with Cosine <em class=\"mw\">Distance</em> being the lowest). We can do that with the following function, return the top <em class=\"mw\">n</em> most similar topics.</p><pre class=\"na nb nc nd ne qk qj ql bq qm bc bl\"><span class=\"qn nw gq qj b bh qo qp m qq qr\" id=\"8719\"># this is a HuggingFace Model<br/>from sentence_transformers import SentenceTransformer<br/>model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')<br/><br/>def get_most_similar_strings(reference_string: str, candidates_list: list[str], n = 10):<br/>    \"\"\"<br/>    Identifies the most similar strings to a reference string from a list of candidate strings.<br/><br/>    This function computes the similarity between a reference string and each string in the candidate list. <br/>    It uses a model to generate embeddings for the reference and candidate strings, and then calculates <br/>    the cosine similarity between the reference embedding and each candidate embedding. The function <br/>    returns the top 'n' most similar strings and their similarity scores.<br/><br/>    Parameters<br/>    ----------<br/>    reference_string : str<br/>        The reference string to which the similarity of candidate strings is to be compared.<br/><br/>    candidates_list : list[str]<br/>        A list of candidate strings from which the most similar ones to the reference string are identified.<br/><br/>    n : int, optional<br/>        The number of most similar strings to return. Defaults to 10.<br/><br/>    Returns<br/>    -------<br/>    tuple of (list, list)<br/>        A tuple containing two lists: the first list contains the top 'n' most similar strings from the <br/>        candidates list, and the second list contains their corresponding similarity scores. The similarity <br/>        scores are in the range [0, 1], where 1 indicates perfect similarity.<br/>    \"\"\"<br/>    reference_embedding = model.encode([reference_string])[0]<br/>    encoded_strings = model.encode(candidates_list)<br/>    similarities = [1 - cosine(reference_embedding, encoded_str) for encoded_str in encoded_strings]<br/>    most_similar_indices = np.argsort(similarities)[::-1][:n]<br/>    return [candidates_list[i] for i in most_similar_indices], [similarities[i] for i in most_similar_indices]</span></pre><h3 class=\"pp nw gq bg nx pq pr ps ob pt pu pv of mj pw px py mn pz qa qb mr qc qd qe qf bl\" id=\"53e9\"><strong class=\"an\">Re-Ranking Summary Embeddings</strong></h3><p class=\"pw-post-body-paragraph ly lz gq ma b mb ot md me mf ou mh mi mj ov ml mm mn ow mp mq mr ox mt mu mv gj bl\" id=\"a6ea\">At each page, when returning all links on the page, there are sometimes hundreds. These <em class=\"mw\">n</em> links are returned at once, regardless of the size (the time is constant), but to get a summary of each topic, the total time is linear. If there are 50 topics, it would take <em class=\"mw\">n + n * k</em> time to get summaries for all topics on the page (where <em class=\"mw\">k = time per summary retrieval</em>). With 50 topics, assuming <em class=\"mw\">n </em>and<em class=\"mw\"> k</em> are each <em class=\"mw\">1 second</em>, <em class=\"mw\">1 + 1*50 = 51 seconds</em>. The thing is, if a topic is the 40th most relevant when relevancy is determined by only the topic term (without further context), it is unlikely that it will suddenly jump to being the most relevant topic, with context. Assuming this is generally true, we should be safe to only retrieve summaries for a subset of the most relevant topics (without context). If we only retrieved half of the most relevant <em class=\"mw\">n</em> topics, we could nearly cut our processing time in half, modelling the time with <em class=\"mw\">n + (n * k / 2)</em>. This would cut our original 51 seconds down to <em class=\"mw\">1 + (1*50 / 2) = 26 seconds</em>, likely without a noticeable change in performance, since we\u2019re eliminating the summaries of the least relevant topics. To implement this, I have two instances of generating word embeddings. The first time, to generate and compare the embeddings of all <em class=\"mw\">n</em> valid links to the target topic to do a preliminary ranking of relevancy. With this ranking, I take half of the top <em class=\"mw\">n</em> results and get summaries for each of them, and then again generate word embeddings and re-rank these top <em class=\"mw\">n / 2</em> summary embeddings in order of relevancy to the target\u2019s summary.</p></div></div></div><div class=\"ac ci oy oz pa pb\" role=\"separator\"><span class=\"pc by bn pd pe pf\"></span><span class=\"pc by bn pd pe pf\"></span><span class=\"pc by bn pd pe\"></span></div><div class=\"gj gk gl gm gn\"><div class=\"ac ci\"><div class=\"cp bi fv fw fx fy\"><h2 class=\"nv nw gq bg nx ny pg oa ob oc ph oe of og pi oi oj ok pj om on oo pk oq or os bl\" id=\"95a4\">(While) Looping Until Complete</h2><p class=\"pw-post-body-paragraph ly lz gq ma b mb ot md me mf ou mh mi mj ov ml mm mn ow mp mq mr ox mt mu mv gj bl\" id=\"8000\">Given everything we\u2019ve now talked about, here\u2019s a review of how we put it all together:</p><ul class=\"\"><li class=\"ly lz gq ma b mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv nm nn no bl\" id=\"1b62\">get start and target pages</li><li class=\"ly lz gq ma b mb nq md me mf nr mh mi mj ns ml mm mn nt mp mq mr nu mt mu mv nm nn no bl\" id=\"df71\">get all links/topics from the starting page (with optional validation to remove unwanted topics)</li><li class=\"ly lz gq ma b mb nq md me mf nr mh mi mj ns ml mm mn nt mp mq mr nu mt mu mv nm nn no bl\" id=\"7aa8\">see which topic is most semantically similar to the target topic (with optional re-ranking based on summary embedding similarities)</li><li class=\"ly lz gq ma b mb nq md me mf nr mh mi mj ns ml mm mn nt mp mq mr nu mt mu mv nm nn no bl\" id=\"ac2c\">navigate to that page and continue to loop until the current page topic == target topic</li></ul></div></div></div><div class=\"ac ci oy oz pa pb\" role=\"separator\"><span class=\"pc by bn pd pe pf\"></span><span class=\"pc by bn pd pe pf\"></span><span class=\"pc by bn pd pe\"></span></div><div class=\"gj gk gl gm gn\"><div class=\"ac ci\"><div class=\"cp bi fv fw fx fy\"><h2 class=\"nv nw gq bg nx ny pg oa ob oc ph oe of og pi oi oj ok pj om on oo pk oq or os bl\" id=\"5e4c\">Conclusion</h2><p class=\"pw-post-body-paragraph ly lz gq ma b mb ot md me mf ou mh mi mj ov ml mm mn ow mp mq mr ox mt mu mv gj bl\" id=\"c4d8\">Broadly speaking, this is how my code is structured. To see the coded implementation in full, visit the GitHub repository I\u2019ve made for this project, and the corresponding Streamlit app to test it out for yourself. If you have any questions or comments about this post or would like to get in touch with me for another reason, check below.</p><ul class=\"\"><li class=\"ly lz gq ma b mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv nm nn no bl\" id=\"22d0\">Project Streamlit app: <a class=\"ah np\" href=\"https://wikigamebot.streamlit.app/\" rel=\"noopener ugc nofollow\" target=\"_blank\">https://wikigamebotbot.streamlit.app/</a></li><li class=\"ly lz gq ma b mb nq md me mf nr mh mi mj ns ml mm mn nt mp mq mr nu mt mu mv nm nn no bl\" id=\"12f1\">Project Github Repository: <a class=\"ah np\" href=\"https://github.com/kmaurinjones/WikiGameBot/tree/main\" rel=\"noopener ugc nofollow\" target=\"_blank\">https://github.com/kmaurinjones/WikiGameBot/tree/main</a></li><li class=\"ly lz gq ma b mb nq md me mf nr mh mi mj ns ml mm mn nt mp mq mr nu mt mu mv nm nn no bl\" id=\"5570\">Email: <a class=\"ah np\" href=\"mailto:kmaurinjones@gmail.com\" rel=\"noopener ugc nofollow\" target=\"_blank\">kmaurinjones@gmail.com</a></li><li class=\"ly lz gq ma b mb nq md me mf nr mh mi mj ns ml mm mn nt mp mq mr nu mt mu mv nm nn no bl\" id=\"01a5\">LinkedIn: <a class=\"ah np\" href=\"https://www.linkedin.com/in/kmaurinjones/\" rel=\"noopener ugc nofollow\" target=\"_blank\">https://www.linkedin.com/in/kmaurinjones/</a></li></ul></div></div></div></div></section></div></div></article>"}
    </div>
  </article>
</div>
