<script lang="ts">
  import ArticleLayout from '$lib/components/ArticleLayout.svelte';

  const metadata = {
    title: "Accelerating Home Building Code Research with Retrieval-Augmented Generation (RAG)",
    date: "February 18, 2025",
    categories: ["retrieval augmented gen", "large language models", "ai", "artificial intelligence", "openai"],
    mediumUrl: "https://medium.com/@kmaurinjones/accelerating-building-code-research-with-rag-an-in-depth-look-c6c89d811f23"
  };

  const articleHtml = "<article class=\"meteredContent\"><div class=\"m\"><div class=\"m\"><span class=\"m\"></span><section><div><div class=\"er ge gf gg gh gi\"></div><div></div><div class=\"gn go gp gq gr\"><div class=\"ac ci\"><div class=\"cp bi fv fw fx fy\"><div><div></div></div><p class=\"pw-post-body-paragraph lv lw gu lx b ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms gn bl\" id=\"3852\"></p><figure class=\"mx my mz na nb nc mu mv paragraph-image\"><div class=\"nd ne eo nf bi ng\" role=\"button\" tabindex=\"0\"><div class=\"mu mv mw\"><picture><source sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\" srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*b87HItOf6W_4bMtws9FhkQ.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*b87HItOf6W_4bMtws9FhkQ.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*b87HItOf6W_4bMtws9FhkQ.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*b87HItOf6W_4bMtws9FhkQ.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*b87HItOf6W_4bMtws9FhkQ.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*b87HItOf6W_4bMtws9FhkQ.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b87HItOf6W_4bMtws9FhkQ.jpeg 1400w\" type=\"image/webp\"/><source data-testid=\"og\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*b87HItOf6W_4bMtws9FhkQ.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*b87HItOf6W_4bMtws9FhkQ.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*b87HItOf6W_4bMtws9FhkQ.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*b87HItOf6W_4bMtws9FhkQ.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*b87HItOf6W_4bMtws9FhkQ.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*b87HItOf6W_4bMtws9FhkQ.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*b87HItOf6W_4bMtws9FhkQ.jpeg 1400w\"/><img alt=\"\" class=\"bi lf nh c\" height=\"400\" loading=\"eager\" role=\"presentation\" width=\"700\"/></picture></div></div><figcaption class=\"ni nj nk mu mv nl nm bg b bh ab ee\">Image by author. Generated using <a class=\"ah nn\" href=\"https://replicate.com/black-forest-labs/flux-1.1-pro\" rel=\"noopener ugc nofollow\" target=\"_blank\">FLUX-1.1-Pro</a>.</figcaption></figure></div></div></div><div class=\"ac ci gj no np nq\" role=\"separator\"><span class=\"nr by bn ns nt nu\"></span><span class=\"nr by bn ns nt nu\"></span><span class=\"nr by bn ns nt\"></span></div><div class=\"gn go gp gq gr\"><div class=\"ac ci\"><div class=\"cp bi fv fw fx fy\"><p class=\"pw-post-body-paragraph lv lw gu lx b ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms gn bl\" id=\"0399\">Over the weekend, a friend reached out to me, asking if I could help them \u2018search\u2019 the Ontario Building Code \u2014 a document spanning nearly 800,000 words \u2014 more effectively. As it currently stands, for someone to answer a question they have about the Code, every lookup would involve carefully crafted search queries using logical operators that often returned hundreds of results, making each search take anywhere from 10 to 30 minutes. By integrating a Retrieval-Augmented Generation (RAG) system, I transformed this process into an almost instantaneous, conversational experience.</p><p class=\"pw-post-body-paragraph lv lw gu lx b ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms gn bl\" id=\"f0bd\">In this article, I\u2019ll dive into the technical details and design decisions that I used in this project.</p></div></div><div class=\"nc\"><div class=\"ac ci\"><div class=\"la nv lb nw lc nx cm ny cn nz cp bi\"><figure class=\"mx my mz na nb nc ob oc paragraph-image\"><div class=\"nd ne eo nf bi ng\" role=\"button\" tabindex=\"0\"><div class=\"mu mv oa\"><picture><source sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 1000px\" srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*vbZs5KZIR2K-0oXZJGBTyQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*vbZs5KZIR2K-0oXZJGBTyQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*vbZs5KZIR2K-0oXZJGBTyQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*vbZs5KZIR2K-0oXZJGBTyQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*vbZs5KZIR2K-0oXZJGBTyQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*vbZs5KZIR2K-0oXZJGBTyQ.png 1100w, https://miro.medium.com/v2/resize:fit:2000/format:webp/1*vbZs5KZIR2K-0oXZJGBTyQ.png 2000w\" type=\"image/webp\"/><source data-testid=\"og\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 1000px\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*vbZs5KZIR2K-0oXZJGBTyQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*vbZs5KZIR2K-0oXZJGBTyQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*vbZs5KZIR2K-0oXZJGBTyQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*vbZs5KZIR2K-0oXZJGBTyQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*vbZs5KZIR2K-0oXZJGBTyQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*vbZs5KZIR2K-0oXZJGBTyQ.png 1100w, https://miro.medium.com/v2/resize:fit:2000/1*vbZs5KZIR2K-0oXZJGBTyQ.png 2000w\"/><img alt=\"\" class=\"bi lf nh c\" height=\"516\" loading=\"lazy\" role=\"presentation\" width=\"1000\"/></picture></div></div><figcaption class=\"ni nj nk mu mv nl nm bg b bh ab ee\">Screenshot of app in-use. Image by author.</figcaption></figure></div></div></div></div><div class=\"ac ci gj no np nq\" role=\"separator\"><span class=\"nr by bn ns nt nu\"></span><span class=\"nr by bn ns nt nu\"></span><span class=\"nr by bn ns nt\"></span></div><div class=\"gn go gp gq gr\"><div class=\"ac ci\"><div class=\"cp bi fv fw fx fy\"><h2 class=\"od oe gu bg of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz pa bl\" id=\"4887\">Retrieval-Augmented Generation</h2><h3 class=\"pb oe gu bg of pc pd pe oj pf pg ph on mg pi pj pk mk pl pm pn mo po pp pq pr bl\" id=\"e010\">What is RAG?</h3><p class=\"pw-post-body-paragraph lv lw gu lx b ly ps ma mb mc pt me mf mg pu mi mj mk pv mm mn mo pw mq mr ms gn bl\" id=\"fe67\">Retrieval-Augmented Generation (RAG) is a way to process high volumes of data, (specifically \u2014 larger than the context window of a given Large Language Model (LLM)), such that the most salient points of the data are returned and can be shown to the model for a question-and-answer user interaction. It\u2019s most popular in chatbot applications.</p><h3 class=\"pb oe gu bg of pc pd pe oj pf pg ph on mg pi pj pk mk pl pm pn mo po pp pq pr bl\" id=\"7049\">Why RAG?</h3><p class=\"pw-post-body-paragraph lv lw gu lx b ly ps ma mb mc pt me mf mg pu mi mj mk pv mm mn mo pw mq mr ms gn bl\" id=\"42ee\">RAG has been a hot topic for the past few years in the world of LLMs, sparked by a fundamental challenge in chatbot applications: many use cases require inference over volumes of data that <em class=\"mt\">exceed</em> the model\u2019s context window limitations. In such scenarios, we cannot simply pass the entire document to the model as immediate context for queries.</p><p class=\"pw-post-body-paragraph lv lw gu lx b ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms gn bl\" id=\"c01e\">In theory, RAG is the ideal solution to this because it aims to return only the necessary subset of information (\u2018chunks\u2019) from the source document(s), such that this subset is within the model\u2019s context window, but still sufficiently provides semantic context to the user\u2019s query that the question can be answered successfully by the LLM.</p><p class=\"pw-post-body-paragraph lv lw gu lx b ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms gn bl\" id=\"4707\">However, it\u2019s important to note that RAG is not always the optimal solution. In some information retrieval contexts, a RAG implementation like the one described in this application may not perform well. This is primarily due to the simplicity of the implementation and its underlying information retrieval logic, which assumes the source content is highly structured and lacks significant continuity across the document.</p><p class=\"pw-post-body-paragraph lv lw gu lx b ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms gn bl\" id=\"61e9\">In this particular example, while there is some topical continuity across different sections of the document, it is less pronounced compared to more narrative-style content. For instance, if the document were a storybook, chunking it into discrete sections and returning them in an unordered, non-sequential fashion would result in a loss of narrative continuity between chunks. While I attempted to mitigate this risk by implementing overlap tokens during the chunking process, maintaining perfect continuity cannot be guaranteed.</p><h2 class=\"od oe gu bg of og px oi oj ok py om on oo pz oq or os qa ou ov ow qb oy oz pa bl\" id=\"0f09\">Source Content Structure</h2><p class=\"pw-post-body-paragraph lv lw gu lx b ly ps ma mb mc pt me mf mg pu mi mj mk pv mm mn mo pw mq mr ms gn bl\" id=\"5f50\">The Ontario Building Code is naturally segmented into sections, subsections, and tables. This clear structure is leveraged to optimize retrieval.</p><h3 class=\"pb oe gu bg of pc pd pe oj pf pg ph on mg pi pj pk mk pl pm pn mo po pp pq pr bl\" id=\"ecb4\"><strong class=\"an\">Segmented Content</strong></h3><p class=\"pw-post-body-paragraph lv lw gu lx b ly ps ma mb mc pt me mf mg pu mi mj mk pv mm mn mo pw mq mr ms gn bl\" id=\"ee89\">The document is divided into discrete, logically organized sections. This segmentation allows each section to be processed independently, reducing the risk of losing context when only partial content is retrieved.</p><h3 class=\"pb oe gu bg of pc pd pe oj pf pg ph on mg pi pj pk mk pl pm pn mo po pp pq pr bl\" id=\"4638\"><strong class=\"an\">Efficient Chunking</strong></h3><p class=\"pw-post-body-paragraph lv lw gu lx b ly ps ma mb mc pt me mf mg pu mi mj mk pv mm mn mo pw mq mr ms gn bl\" id=\"5172\">The full text is split into token-based chunks (chunk size = 1000 tokens). This approach uses overlapping segments (overlap = 200 tokens) to ensure that each chunk is self-contained yet sufficiently connected to its neighbours, preserving context without overwhelming the model.</p><h3 class=\"pb oe gu bg of pc pd pe oj pf pg ph on mg pi pj pk mk pl pm pn mo po pp pq pr bl\" id=\"5290\"><strong class=\"an\">Caching and Updates</strong></h3><p class=\"pw-post-body-paragraph lv lw gu lx b ly ps ma mb mc pt me mf mg pu mi mj mk pv mm mn mo pw mq mr ms gn bl\" id=\"e1de\">A caching mechanism within the app stores the retrieved content along with metadata (such as the last update timestamp). This ensures that the system only re-scrapes the content when necessary, balancing freshness with efficiency. This is not something I spent a lot of time on, and this of course could be done better if this project were to be scaled up, but this was sufficiently functional for the end user base and the time I wanted to spend on this project.</p></div></div></div><div class=\"ac ci gj no np nq\" role=\"separator\"><span class=\"nr by bn ns nt nu\"></span><span class=\"nr by bn ns nt nu\"></span><span class=\"nr by bn ns nt\"></span></div><div class=\"gn go gp gq gr\"><div class=\"ac ci\"><div class=\"cp bi fv fw fx fy\"><h2 class=\"od oe gu bg of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz pa bl\" id=\"2191\">Enhancing User Queries with Intelligent Expansion</h2><p class=\"pw-post-body-paragraph lv lw gu lx b ly ps ma mb mc pt me mf mg pu mi mj mk pv mm mn mo pw mq mr ms gn bl\" id=\"b085\">User queries are often brief and ambiguous. To ensure effective retrieval, the system employs a query expansion module that transforms the initial query into multiple, targeted search queries.</p><h3 class=\"pb oe gu bg of pc pd pe oj pf pg ph on mg pi pj pk mk pl pm pn mo po pp pq pr bl\" id=\"233c\"><strong class=\"an\">Contextual Awareness</strong></h3><p class=\"pw-post-body-paragraph lv lw gu lx b ly ps ma mb mc pt me mf mg pu mi mj mk pv mm mn mo pw mq mr ms gn bl\" id=\"42f7\">The system factors in previous interactions to generate search queries that cover different facets of the question, avoiding redundancy.</p><h3 class=\"pb oe gu bg of pc pd pe oj pf pg ph on mg pi pj pk mk pl pm pn mo po pp pq pr bl\" id=\"7eb1\"><strong class=\"an\">Diverse Query Generation</strong></h3><p class=\"pw-post-body-paragraph lv lw gu lx b ly ps ma mb mc pt me mf mg pu mi mj mk pv mm mn mo pw mq mr ms gn bl\" id=\"717e\">By expanding a single query into several focused search terms, the system increases the likelihood of retrieving the exact sections needed.</p><h3 class=\"pb oe gu bg of pc pd pe oj pf pg ph on mg pi pj pk mk pl pm pn mo po pp pq pr bl\" id=\"f56f\">Example</h3><p class=\"pw-post-body-paragraph lv lw gu lx b ly ps ma mb mc pt me mf mg pu mi mj mk pv mm mn mo pw mq mr ms gn bl\" id=\"62ff\">For instance, if the user submits the chat message \u201cI\u2019m interested in fire safety stuff in the house I\u2019m building\u201d, in a keyword-based search system, the results would be awful. The words \u201cstuff\u201d, \u201cI\u2019m\u201d, and \u201chouse\u201d would respectively return nothing, return only other noise, or not return anything, as each of these terms is either unrelated or too generic and not specifically enough related to \u2018fire safety\u2019. To make is \u2018possible\u2019 for the user to submit queries like and still have good retrieval and response performance, query expansion aims to generate RAG-friendly extrapolations of the user\u2019s passed query. These could look like:</p><ul class=\"\"><li class=\"lv lw gu lx b ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms qc qd qe bl\" id=\"fde9\"><em class=\"mt\">Fire exit requirements Ontario Building Code</em></li><li class=\"lv lw gu lx b ly qf ma mb mc qg me mf mg qh mi mj mk qi mm mn mo qj mq mr ms qc qd qe bl\" id=\"d2a4\"><em class=\"mt\">Emergency lighting regulations in building codes</em></li><li class=\"lv lw gu lx b ly qf ma mb mc qg me mf mg qh mi mj mk qi mm mn mo qj mq mr ms qc qd qe bl\" id=\"9763\"><em class=\"mt\">Fire safety measures for residential construction</em></li></ul><h3 class=\"pb oe gu bg of pc pd pe oj pf pg ph on mg pi pj pk mk pl pm pn mo po pp pq pr bl\" id=\"c134\"><strong class=\"an\">Token Efficiency</strong></h3><p class=\"pw-post-body-paragraph lv lw gu lx b ly ps ma mb mc pt me mf mg pu mi mj mk pv mm mn mo pw mq mr ms gn bl\" id=\"59ed\">The expansion module is designed to be optional. When the original query is clear, it can bypass this step, saving tokens \u2014 and thereby cost \u2014 over time.</p></div></div></div><div class=\"ac ci gj no np nq\" role=\"separator\"><span class=\"nr by bn ns nt nu\"></span><span class=\"nr by bn ns nt nu\"></span><span class=\"nr by bn ns nt\"></span></div><div class=\"gn go gp gq gr\"><div class=\"ac ci\"><div class=\"cp bi fv fw fx fy\"><h2 class=\"od oe gu bg of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz pa bl\" id=\"6593\">Optimizations &amp; Notable Features</h2><p class=\"pw-post-body-paragraph lv lw gu lx b ly ps ma mb mc pt me mf mg pu mi mj mk pv mm mn mo pw mq mr ms gn bl\" id=\"3c37\">The system is optimized for both speed and cost-efficiency.</p><h3 class=\"pb oe gu bg of pc pd pe oj pf pg ph on mg pi pj pk mk pl pm pn mo po pp pq pr bl\" id=\"7a2c\"><strong class=\"an\">Lean Context Integration</strong></h3><ul class=\"\"><li class=\"lv lw gu lx b ly ps ma mb mc pt me mf mg pu mi mj mk pv mm mn mo pw mq mr ms qc qd qe bl\" id=\"b1af\">Only the current query and its directly retrieved context are sent to the language model.</li><li class=\"lv lw gu lx b ly qf ma mb mc qg me mf mg qh mi mj mk qi mm mn mo qj mq mr ms qc qd qe bl\" id=\"74fc\">Historical RAG context is removed from the chatbot\u2019s maintain conversation context once the response to the most recent user query is generated, significantly reducing the token volume of each API call turn-over-turn. This also helps to improve performance in model response quality, as this prevents excessive data noise (previously used RAG chunks) from past conversation turns.</li></ul><h3 class=\"pb oe gu bg of pc pd pe oj pf pg ph on mg pi pj pk mk pl pm pn mo po pp pq pr bl\" id=\"896c\"><strong class=\"an\">Real-Time Token Management</strong></h3><ul class=\"\"><li class=\"lv lw gu lx b ly ps ma mb mc pt me mf mg pu mi mj mk pv mm mn mo pw mq mr ms qc qd qe bl\" id=\"c848\">Every step, from query expansion to final response generation, is monitored with token counters.</li><li class=\"lv lw gu lx b ly qf ma mb mc qg me mf mg qh mi mj mk qi mm mn mo qj mq mr ms qc qd qe bl\" id=\"6b30\">A real-time display informs users of input, output, and overall token usage, ensuring transparency about API costs. As this app is self-serve (the user must pass their OpenAI API key), this keeps my own costs low, while still having a neat project deployed and publicly accessible.</li></ul><h3 class=\"pb oe gu bg of pc pd pe oj pf pg ph on mg pi pj pk mk pl pm pn mo po pp pq pr bl\" id=\"e85a\"><strong class=\"an\">Accurate Citations</strong></h3><ul class=\"\"><li class=\"lv lw gu lx b ly ps ma mb mc pt me mf mg pu mi mj mk pv mm mn mo pw mq mr ms qc qd qe bl\" id=\"6b1a\">Responses are generated with strict prompting to always include citations. Of course, there are ways to more explicitly ensure (or mitigate the risk of not providing) citations, but again \u2014 given the time I wanted to spend on this project, this is what I felt was a healthy balance of development time for me, versus results quality.</li><li class=\"lv lw gu lx b ly qf ma mb mc qg me mf mg qh mi mj mk qi mm mn mo qj mq mr ms qc qd qe bl\" id=\"fc31\">References (e.g., <strong class=\"lx gv\">Section 1.2.3</strong>) and snippets of the exact text are provided as much as possible, allowing users to verify answers by cross-referencing the official code.</li></ul><h3 class=\"pb oe gu bg of pc pd pe oj pf pg ph on mg pi pj pk mk pl pm pn mo po pp pq pr bl\" id=\"af4b\"><strong class=\"an\">Performance Enhancements</strong></h3><ul class=\"\"><li class=\"lv lw gu lx b ly ps ma mb mc pt me mf mg pu mi mj mk pv mm mn mo pw mq mr ms qc qd qe bl\" id=\"693c\">Expensive operations, such as initializing the vector store and query expansion module, are cached.</li><li class=\"lv lw gu lx b ly qf ma mb mc qg me mf mg qh mi mj mk qi mm mn mo qj mq mr ms qc qd qe bl\" id=\"f020\">This caching minimizes startup delays and reduces redundant calls, making the system more responsive during prolonged interactions.</li></ul></div></div></div><div class=\"ac ci gj no np nq\" role=\"separator\"><span class=\"nr by bn ns nt nu\"></span><span class=\"nr by bn ns nt nu\"></span><span class=\"nr by bn ns nt\"></span></div><div class=\"gn go gp gq gr\"><div class=\"ac ci\"><div class=\"cp bi fv fw fx fy\"><h2 class=\"od oe gu bg of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz pa bl\" id=\"a6bf\">Architecture and Workflow</h2><h3 class=\"pb oe gu bg of pc pd pe oj pf pg ph on mg pi pj pk mk pl pm pn mo po pp pq pr bl\" id=\"d428\"><strong class=\"an\">Vector-Based Retrieval</strong></h3><ul class=\"\"><li class=\"lv lw gu lx b ly ps ma mb mc pt me mf mg pu mi mj mk pv mm mn mo pw mq mr ms qc qd qe bl\" id=\"8dd0\">A vector database efficiently indexes the tokenized chunks.</li><li class=\"lv lw gu lx b ly qf ma mb mc qg me mf mg qh mi mj mk qi mm mn mo qj mq mr ms qc qd qe bl\" id=\"fb82\">When a query is issued, the system generates embedding vectors for the user query, and each expanded query, and proceeds retrieves the top <em class=\"mt\">k </em>relevant chunks (set to 3 in this implementation) using Cosine Similarity to compare the respective query and chunk embeddings. This is accomplished using a vector store that leverages a fast indexing engine for near-instant results.</li></ul><h3 class=\"pb oe gu bg of pc pd pe oj pf pg ph on mg pi pj pk mk pl pm pn mo po pp pq pr bl\" id=\"db4a\"><strong class=\"an\">Conversational Interface</strong></h3><ul class=\"\"><li class=\"lv lw gu lx b ly ps ma mb mc pt me mf mg pu mi mj mk pv mm mn mo pw mq mr ms qc qd qe bl\" id=\"88eb\">The user interface is designed for smooth, interactive conversation.</li><li class=\"lv lw gu lx b ly qf ma mb mc qg me mf mg qh mi mj mk qi mm mn mo qj mq mr ms qc qd qe bl\" id=\"6260\">When a query is submitted, the system immediately \u2018expands\u2019 it, retrieves related sections, and compiles a rich context for the model.</li><li class=\"lv lw gu lx b ly qf ma mb mc qg me mf mg qh mi mj mk qi mm mn mo qj mq mr ms qc qd qe bl\" id=\"e534\">Responses are streamed back token-by-token, providing real-time feedback and an engaging user experience.</li><li class=\"lv lw gu lx b ly qf ma mb mc qg me mf mg qh mi mj mk qi mm mn mo qj mq mr ms qc qd qe bl\" id=\"c229\">As each of these steps occur in the conversation, the user is shown the process and can see further information about the generated queries, and the context retrieved in the RAG component.</li></ul></div></div></div><div class=\"ac ci gj no np nq\" role=\"separator\"><span class=\"nr by bn ns nt nu\"></span><span class=\"nr by bn ns nt nu\"></span><span class=\"nr by bn ns nt\"></span></div><div class=\"gn go gp gq gr\"><div class=\"ac ci\"><div class=\"cp bi fv fw fx fy\"><h2 class=\"od oe gu bg of og oh oi oj ok ol om on oo op oq or os ot ou ov ow ox oy oz pa bl\" id=\"fcc5\">Wrapping Up</h2><p class=\"pw-post-body-paragraph lv lw gu lx b ly ps ma mb mc pt me mf mg pu mi mj mk pv mm mn mo pw mq mr ms gn bl\" id=\"f428\">By blending these techniques \u2014 a well-structured content organization, intelligent query expansion, and chat context management \u2014 navigating large and hard-to-search documents can become a relatively straightforward task.</p><p class=\"pw-post-body-paragraph lv lw gu lx b ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms gn bl\" id=\"f339\">For anyone interested in applying similar techniques to other domains, this project demonstrates that \u2018simple\u2019 is not necessarily bad design. Thoughtfully applied, simple engineering choices can make a big impact.</p><ul class=\"\"><li class=\"lv lw gu lx b ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms qc qd qe bl\" id=\"aa52\"><strong class=\"lx gv\">GitHub Repository: </strong><a class=\"ah nn\" href=\"https://github.com/kmaurinjones/Ontario-Building-Code-Chat\" rel=\"noopener ugc nofollow\" target=\"_blank\">Ontario Building Code Chat</a></li><li class=\"lv lw gu lx b ly qf ma mb mc qg me mf mg qh mi mj mk qi mm mn mo qj mq mr ms qc qd qe bl\" id=\"7acf\"><strong class=\"lx gv\">Test the App: </strong><a class=\"ah nn\" href=\"https://buildy.streamlit.app/\" rel=\"noopener ugc nofollow\" target=\"_blank\">https://buildy.streamlit.app/</a></li><li class=\"lv lw gu lx b ly qf ma mb mc qg me mf mg qh mi mj mk qi mm mn mo qj mq mr ms qc qd qe bl\" id=\"3b39\"><strong class=\"lx gv\">Source Content: </strong><a class=\"ah nn\" href=\"https://www.ontario.ca/laws/regulation/120332/v25\" rel=\"noopener ugc nofollow\" target=\"_blank\">Ontario Building Code v25</a></li></ul><p class=\"pw-post-body-paragraph lv lw gu lx b ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms gn bl\" id=\"e19e\">Feel free to explore, contribute, or reach out if you have questions about implementing similar solutions.</p><p class=\"pw-post-body-paragraph lv lw gu lx b ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms gn bl\" id=\"62d7\">Email: kmaurinjones@gmail.com</p><p class=\"pw-post-body-paragraph lv lw gu lx b ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms gn bl\" id=\"f50f\">LinkedIn: <a class=\"ah nn\" href=\"https://www.linkedin.com/in/kmaurinjones/\" rel=\"noopener ugc nofollow\" target=\"_blank\">https://www.linkedin.com/in/kmaurinjones/</a></p></div></div></div></div></section></div></div></article>";
</script>

<ArticleLayout
  title={metadata.title}
  date={metadata.date}
  categories={metadata.categories}
>
  {@html articleHtml}
</ArticleLayout>
